{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to c:\\users\\aashutosh raj\\appdata\\local\\temp\\pip-req-build-5f26i2t_\n",
      "Requirement already satisfied: nltk in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.6.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (2.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (0.24.1)\n",
      "Requirement already satisfied: unidecode in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.3.7)\n",
      "Requirement already satisfied: future in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (0.18.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (1.0.1)\n",
      "Requirement already satisfied: spacy>=3.2.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pke==2.0.0) (3.7.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (0.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (20.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.10.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx->pke==2.0.0) (3.3.4)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx->pke==2.0.0) (1.2.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx->pke==2.0.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx->pke==2.0.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx->pke==2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx->pke==2.0.0) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.3->networkx->pke==2.0.0) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pandas>=1.1->networkx->pke==2.0.0) (2021.1)\n",
      "Requirement already satisfied: regex in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from nltk->pke==2.0.0) (2021.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from scikit-learn->pke==2.0.0) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/boudinfl/pke.git 'C:\\Users\\AASHUTOSH RAJ\\AppData\\Local\\Temp\\pip-req-build-5f26i2t_'\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet flashtext==2.7\n",
    "!pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.28.1\n",
    "!pip install --quiet sentencepiece==0.1.95\n",
    "!pip install --quiet textwrap3==0.9.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet strsim==0.0.3\n",
    "!pip install --quiet sense2vec==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-10-07 22:48:17 +05:30)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.36 s (started: 2024-10-07 22:48:26 +05:30)\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.8.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from scipy==1.8.0) (1.20.1)\n",
      "Requirement already satisfied: networkx==2.6 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (2.6)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx==2.6) (1.20.1)\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx==2.6) (1.2.4)\n",
      "Requirement already satisfied: scipy!=1.6.1,>=1.5 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx==2.6) (1.8.0)\n",
      "Requirement already satisfied: matplotlib>=3.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from networkx==2.6) (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx==2.6) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx==2.6) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx==2.6) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx==2.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->networkx==2.6) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.3->networkx==2.6) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\aashutosh raj\\anaconda3\\lib\\site-packages (from pandas>=1.1->networkx==2.6) (2021.1)\n",
      "time: 8.59 s (started: 2024-10-07 22:48:40 +05:30)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy==1.8.0\n",
    "!pip install networkx==2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-10-07 22:49:03 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "from textwrap3 import wrap\n",
    "# importing the locale module and setting the getpreferredencoding\n",
    "# function to always return the string \"UTF-8\". After that, it imports the textwrap3 module and specifically imports the wrap function from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk is a business magnate and investor. He is the founder, CEO and chief engineer of SpaceX; angel investor, CEO and product architect of\n",
      "Tesla, Inc.; owner and CTO of Twitter; founder of the Boring Company; co-founder of Neuralink and OpenAI; and president of the philanthropic Musk\n",
      "Foundation.\n",
      "\n",
      "\n",
      "time: 0 ns (started: 2024-10-07 22:49:15 +05:30)\n"
     ]
    }
   ],
   "source": [
    "text = \"Elon Reeve Musk is a business magnate and investor. He is the founder, CEO and chief engineer of SpaceX; angel investor, CEO and product architect of Tesla, Inc.; owner and CTO of Twitter; founder of the Boring Company; co-founder of Neuralink and OpenAI; and president of the philanthropic Musk Foundation.\"\n",
    "for wrp in wrap(text, 150):\n",
    "  print (wrp)\n",
    "print (\"\\n\")\n",
    "#  processing a text string and using the wrap function from the textwrap3\n",
    "#  module to format and wrap the text into lines of a specified maximum width (150 characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.1 s (started: 2024-10-07 22:49:30 +05:30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AASHUTOSH RAJ\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summary_model = summary_model.to(device)\n",
    "# sets up a T5 model for text summarization, loads a pre-trained model and tokenizer, and determines whether to use a GPU for accelerated processing if one is available.\n",
    "# This code prepares the model and environment for text summarization tasks, which can involve input text and outputting a concise summary of that text using the T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32 ms (started: 2024-10-07 22:50:11 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "# function set_seed(42) is called with a specific seed value, in this case, 42. Setting a seed value of 42 (or any other value) will make the random number generation\n",
    "# consistent and reproducible across different runs of the program.\n",
    "# This is particularly important in machine learning and deep learning where randomness can affect the results,\n",
    "# and having reproducible results for experimentation and debugging is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AASHUTOSH\n",
      "[nltk_data]     RAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to C:\\Users\\AASHUTOSH\n",
      "[nltk_data]     RAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\AASHUTOSH\n",
      "[nltk_data]     RAJ\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original Text >>\n",
      "summarize:Elon Reeve Musk is a business magnate and investor. He is the founder, CEO and chief engineer of SpaceX; angel investor, CEO and product\n",
      "architect of Tesla, Inc.; owner and CTO of Twitter; founder of the Boring Company; co-founder of Neuralink and OpenAI; and president of the\n",
      "philanthropic Musk Foundation.\n",
      "\n",
      "\n",
      "Summarized Text >>\n",
      "Elon reeve musk is a business magnate and investor. He is the founder, ceo and chief engineer of spacex ; founder of the boring company; co-founder of\n",
      "neuralink and openai; and president of philanthropic musk foundation. His most recent book, 'the frog', was published in january.\n",
      "\n",
      "\n",
      "time: 20.9 s (started: 2024-10-07 22:50:30 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def postprocesstext (content):\n",
    "  final=\"\"\n",
    "  for sent in sent_tokenize(content):\n",
    "    sent = sent.capitalize()\n",
    "    final = final +\" \"+sent\n",
    "  return final\n",
    "\n",
    "\n",
    "def summarizer(text,model,tokenizer):\n",
    "  text = text.strip().replace(\"\\n\",\" \")\n",
    "  text = \"summarize:\"+text\n",
    "  # print (text)\n",
    "  max_len = 512\n",
    "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "  outs = model.generate(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  early_stopping=True,\n",
    "                                  num_beams=3,\n",
    "                                  num_return_sequences=1,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  min_length = 75,\n",
    "                                  max_length=300)\n",
    "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
    "  summary = dec[0]\n",
    "  summary = postprocesstext(summary)\n",
    "  summary= summary.strip()\n",
    "\n",
    "  return summary\n",
    "text = text.strip().replace(\"\\n\",\" \")\n",
    "text = \"summarize:\"+text\n",
    "\n",
    "summarized_text = summarizer(text,summary_model,summary_tokenizer)\n",
    "\n",
    "print (\"\\noriginal Text >>\")\n",
    "for wrp in wrap(text, 150):\n",
    "  print (wrp)\n",
    "print (\"\\n\")\n",
    "print (\"Summarized Text >>\")\n",
    "for wrp in wrap(summarized_text, 150):\n",
    "  print (wrp)\n",
    "print (\"\\n\")\n",
    "# In summary, this code sets up a text summarization pipeline using a pre-trained T5 model, NLTK for text processing,\n",
    "# and custom functions for post-processing. It takes an input text, generates a summary, and prints both the original text and the summary.\n",
    "# This code can be used for summarizing long pieces of text into more concise versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AASHUTOSH\n",
      "[nltk_data]     RAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.33 s (started: 2024-10-07 22:51:14 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pke\n",
    "import traceback\n",
    "def get_nouns_multipartite(content):\n",
    "    out=[]\n",
    "    try:\n",
    "        extractor = pke.unsupervised.MultipartiteRank()\n",
    "        extractor.load_document(input=content,language='en')\n",
    "        #    not contain punctuation marks or stopwords as candidates.\n",
    "        pos = {'PROPN','NOUN'}\n",
    "        #pos = {'PROPN','NOUN'}\n",
    "        stoplist = list(string.punctuation)\n",
    "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "        extractor.candidate_selection(pos=pos)\n",
    "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "        #    threshold/method parameters.\n",
    "        extractor.candidate_weighting(alpha=1.1,\n",
    "                                      threshold=0.75,\n",
    "                                      method='average')\n",
    "        keyphrases = extractor.get_n_best(n=15)\n",
    "        for val in keyphrases:\n",
    "            out.append(val[0])\n",
    "    except:\n",
    "        out = []\n",
    "        traceback.print_exc()\n",
    "    return out\n",
    "    # Python function that uses the \"pke\" (Python Keyphrase Extraction) library to extract keyphrases (important phrases or terms) from a given input text\n",
    "    # his code is a keyphrase extraction function that uses the MultipartiteRank method from the \"pke\" library.\n",
    "    # It processes the input text, selects candidate keyphrases, assigns weights and ranks them, and returns the top keyphrases based on their scores.\n",
    "    # Keyphrase extraction is a common NLP task and is useful for identifying important terms or topics in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords unsummarized:  ['investor', 'ceo', 'founder', 'elon reeve musk', 'business magnate', 'spacex', 'product architect', 'twitter', 'cto', 'engineer', 'angel investor', 'boring company', 'co-founder', 'owner', 'neuralink']\n",
      "keywords_found in summarized:  ['elon reeve musk', 'spacex', 'founder', 'neuralink', 'boring company', 'ceo', 'engineer', 'co-founder', 'investor', 'business magnate']\n",
      "['investor', 'ceo', 'founder', 'elon reeve musk', 'business magnate', 'spacex', 'engineer', 'boring company', 'co-founder', 'neuralink']\n",
      "time: 656 ms (started: 2024-10-07 22:51:39 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "def get_keywords(originaltext,summarytext):\n",
    "  keywords = get_nouns_multipartite(originaltext)\n",
    "  print (\"keywords unsummarized: \",keywords)\n",
    "  keyword_processor = KeywordProcessor()\n",
    "  for keyword in keywords:\n",
    "    keyword_processor.add_keyword(keyword)\n",
    "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
    "  keywords_found = list(set(keywords_found))\n",
    "  print (\"keywords_found in summarized: \",keywords_found)\n",
    "  important_keywords =[]\n",
    "  for keyword in keywords:\n",
    "    if keyword in keywords_found:\n",
    "      important_keywords.append(keyword)\n",
    "  return important_keywords[:10]\n",
    "imp_keywords = get_keywords(text,summarized_text)\n",
    "print (imp_keywords)\n",
    "# function get_keywords that extracts important keywords from the original text and then identifies and selects those keywords that also appear in the summarized text.\n",
    "# identify and return the top 10 important keywords that appear in both the original text and the summarized text.\n",
    "# It can help in summarization evaluation by highlighting the keywords that are successfully captured in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.34 s (started: 2024-10-07 22:51:53 +05:30)\n"
     ]
    }
   ],
   "source": [
    "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
    "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
    "question_model = question_model.to(device)\n",
    "# sets up a T5 model and tokenizer for question generation.\n",
    "# The model is fine-tuned specifically for generating questions based on a given context, making it suitable for tasks like question-answering and information retrieval.\n",
    "# The code also ensures that the model is placed on the appropriate device for efficient computation.\n",
    "# After this setup, you can use this model and tokenizer to generate questions based on provided context or passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon reeve musk is a business magnate and investor. He is the founder, ceo and chief engineer of spacex ; founder of the boring company; co-founder of\n",
      "neuralink and openai; and president of philanthropic musk foundation. His most recent book, 'the frog', was published in january.\n",
      "\n",
      "\n",
      "What is Elon reeve musk's career?\n",
      "Investor\n",
      "\n",
      "\n",
      "What is Elon reeve musk's position at spacex?\n",
      "Ceo\n",
      "\n",
      "\n",
      "What is Elon reeve musk's role in spacex?\n",
      "Founder\n",
      "\n",
      "\n",
      "Who is the founder of spacex?\n",
      "Elon reeve musk\n",
      "\n",
      "\n",
      "What is Elon reeve musk's profession?\n",
      "Business magnate\n",
      "\n",
      "\n",
      "What company is Elon reeve musk the founder of?\n",
      "Spacex\n",
      "\n",
      "\n",
      "What is Elon reeve musk's career?\n",
      "Engineer\n",
      "\n",
      "\n",
      "What company did Elon reeve musk founded?\n",
      "Boring company\n",
      "\n",
      "\n",
      "What is Elon reeve musk's role in neuralink?\n",
      "Co-founder\n",
      "\n",
      "\n",
      "Along with openai, what company did Elon reeve musk co-found?\n",
      "Neuralink\n",
      "\n",
      "\n",
      "time: 37.8 s (started: 2024-10-07 22:52:31 +05:30)\n"
     ]
    }
   ],
   "source": [
    "def get_question(context,answer,model,tokenizer):\n",
    "  text = \"context: {} answer: {}\".format(context,answer)\n",
    "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "  outs = model.generate(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  early_stopping=True,\n",
    "                                  num_beams=5,\n",
    "                                  num_return_sequences=1,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  max_length=72)\n",
    "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
    "  Question = dec[0].replace(\"question:\",\"\")\n",
    "  Question= Question.strip()\n",
    "  return Question\n",
    "for wrp in wrap(summarized_text, 150):\n",
    "  print (wrp)\n",
    "print (\"\\n\")\n",
    "for answer in imp_keywords:\n",
    "  ques = get_question(summarized_text,answer,question_model,question_tokenizer)\n",
    "  print (ques)\n",
    "  print (answer.capitalize())\n",
    "  print (\"\\n\")\n",
    "  # defines a function get_question that generates questions based on a given context and answer, and then it uses this function to generate questions for a list of keywords\n",
    "  # function takes four parameters: context, answer, model, and tokenizer, iterates over the list of important keywords (imp_keywords) and, for each keyword, generates a question.\n",
    "  # the get_question function is called with the summarized text as the context and the keyword as the answer.\n",
    "  # demonstrate how to use a T5 question generation model to generate questions based on keywords extracted from a summarized text.\n",
    "  # The questions are designed to seek information or clarification related to the keywords in the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AASHUTOSH RAJ\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need computers in everything that we use in our daily lives. it becomes very important to make computers intelligent so that our lives become easy.\n",
      "artificial intelligence is the theory and development of computers, which imitates the human intelligence and senses, says dr. sanjay gupta. the u.s.\n",
      "government has backed the use of AI in the country.\n",
      "keywords unsummarized:  ['computers', 'artificial intelligence', 'languages', 'speech recognition', 'translation', 'lives', 'decision-making', 'development', 'theory', 'life', 'perception', 'senses', 'revolution', 'intelligence', 'world']\n",
      "keywords_found in summarized:  ['lives', 'theory', 'senses', 'intelligence', 'artificial intelligence', 'computers', 'development']\n",
      "\n",
      "\n",
      "Noun phrases ['computers', 'artificial intelligence', 'lives', 'development', 'theory', 'senses', 'intelligence']\n",
      "What is the term for the theory and development of computers?\n",
      "Ans:Intelligencewe need computers in everything that we use in our daily lives. it becomes very important to make computers intelligent so that our lives become easy. artificial intelligence is the theory and development of computers, which imitates the human intelligence and senses, says dr. sanjay gupta. the u.s. government has backed the use of AI in the country.\n",
      "time: 1min 59s (started: 2024-10-07 22:53:30 +05:30)\n"
     ]
    }
   ],
   "source": [
    "# same\n",
    "from flashtext import KeywordProcessor\n",
    "# \"flashtext\" library for keyword processing, the \"textwrap3\" library for text wrapping, and the \"transformers\" library for working with transformer-based models like T5.\n",
    "def get_keywords(originaltext,summarytext):\n",
    "  keywords = get_nouns_multipartite(originaltext)\n",
    "  print (\"keywords unsummarized: \",keywords)\n",
    "  keyword_processor = KeywordProcessor()\n",
    "  for keyword in keywords:\n",
    "    keyword_processor.add_keyword(keyword)\n",
    "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
    "  keywords_found = list(set(keywords_found))\n",
    "  print (\"keywords_found in summarized: \",keywords_found)\n",
    "  important_keywords =[]\n",
    "  for keyword in keywords:\n",
    "    if keyword in keywords_found:\n",
    "      important_keywords.append(keyword)\n",
    "  return important_keywords[:10]\n",
    "import torch\n",
    "from textwrap3 import wrap\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summary_model = summary_model.to(device)\n",
    "\n",
    "def summarizer(text,model,tokenizer):\n",
    "  text = text.strip().replace(\"\\n\",\" \")\n",
    "  text = \"summarize:\"+text\n",
    "  # print (text)\n",
    "  max_len = 512\n",
    "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "  outs = model.generate(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  early_stopping=True,\n",
    "                                  num_beams=3,\n",
    "                                  num_return_sequences=1,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  min_length = 75,\n",
    "                                  max_length=300)\n",
    "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
    "  summary = dec[0]\n",
    "  summary= summary.strip()\n",
    "\n",
    "  return summary\n",
    "\n",
    "# same\n",
    "\n",
    "context = input(\"Enter paragraph/content here...\")\n",
    "output = \"\"\n",
    "\n",
    "def generate_question(context):\n",
    "  summary_text = summarizer(context,summary_model,summary_tokenizer)\n",
    "  for wrp in wrap(summary_text, 150):\n",
    "    print (wrp)\n",
    "  np =  get_keywords(context,summary_text)\n",
    "  print (\"\\n\\nNoun phrases\",np)\n",
    "  output=\"\"\n",
    "  for answer in np:\n",
    "    ques = get_question(summary_text,answer,question_model,question_tokenizer)\n",
    "    # output= output + ques + \"\\n\" + \"Ans: \"+answer.capitalize() + \"\\n\\n\"\n",
    "    # output = output + \"<b style='color:blue;'>\" + ques + \"</b>\"\n",
    "    # output = output + \"<br>\"\n",
    "    # output = output + \"<b style='active {color: blue};'>\" + \"Ans: \" +answer.capitalize()+  \"</b>\"\n",
    "    output = ques +\"\\nAns:\" + answer.capitalize()\n",
    "# code initializes a T5 model for text summarization (T5ForConditionalGeneration) and its associated tokenizer (T5Tokenizer).\n",
    "  summary =\"Summary: \"+ summary_text\n",
    "  #for answer in np:\n",
    "    #summary = summary.replace(answer,\"<b>\"+answer+\"</b>\")\n",
    "    #summary = summary.replace(answer.capitalize(),\"<b>\"+answer.capitalize()+\"</b>\")\n",
    "  #output = output + \"<p>\"+summary+\"</p>\"\n",
    "\n",
    "  output += summary_text\n",
    "  print(output)\n",
    "\n",
    "generate_question(context)\n",
    "\n",
    "# code initializes a T5 model for text summarization (T5ForConditionalGeneration) and its associated tokenizer (T5Tokenizer).\n",
    "# checks whether a CUDA-enabled GPU is available and moves the model to the GPU if possible. This sets up the environment for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 's2v_reddit_2015_md.tar.gz'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.2 s (started: 2024-10-07 22:56:33 +05:30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "!tar -xvf  s2v_reddit_2015_md.tar.gz\n",
    "!ls s2v_old\n",
    "# uses the wget utility to download a file from the specified URL\n",
    "#  uses the tar utility to extract the contents of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 16s (started: 2024-10-07 23:00:45 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"s2v_reddit_2015_md.tar.gz\", \"wb\") as file:\n",
    "    file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.8 s (started: 2024-10-07 23:04:29 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"s2v_reddit_2015_md.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall(\"s2v_old\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['._s2v_old', 's2v_old']\n",
      "time: 0 ns (started: 2024-10-07 23:04:53 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"s2v_old\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40 s (started: 2024-10-07 23:07:02 +05:30)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('D:\\SpeechToText\\s2v_old\\s2v_old')\n",
    "# \"sense2vec\" is a library for word and phrase embeddings that captures word senses and their relationships.\n",
    "# s2v variable will contain a loaded Sense2Vec model that can be used for various natural language processing tasks, such as word sense disambiguation, similarity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.1 s (started: 2024-10-07 23:07:49 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# paraphrase-distilroberta-base-v1\n",
    "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "# sentence-transformers\" library provides pre-trained models for generating sentence embeddings, which are vector representations of sentences\n",
    "# line initializes a sentence embedding model by loading a specific pre-trained model.\n",
    "# sentence_transformer_model variable will contain a loaded Sentence Transformer model that can be used to convert text sentences into dense vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 47 ms (started: 2024-10-07 23:09:01 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "# function computes the normalized Levenshtein similarity between two strings, which is a measure of string similarity\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "\n",
    "def filter_same_sense_words(original,wordlist):\n",
    "  # filters a list of words to include only those with the same sense as the base word.\n",
    "  # checks the sense (part-of-speech tag) of the base word\n",
    "  filtered_words=[]\n",
    "  base_sense =original.split('|')[1]\n",
    "  print (base_sense)\n",
    "  for eachword in wordlist:\n",
    "    if eachword[0].split('|')[1] == base_sense:\n",
    "      filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
    "  return filtered_words\n",
    "\n",
    "def get_highest_similarity_score(wordlist,wrd):\n",
    "  # computes the highest similarity score between a given word and a list of words.\n",
    "  # normalized_levenshtein similarity measure to calculate the similarity between the given word and each word in the list.\n",
    "  score=[]\n",
    "  for each in wordlist:\n",
    "    score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
    "  return max(score)\n",
    "\n",
    "def sense2vec_get_words(word,s2v,topn,question):\n",
    "  # sense2vec\" model to obtain a list of words that are semantically related to a given word.\n",
    "  # retrieves the best sense for the input word, considering specific senses like NOUN, PERSON, PRODUCT, etc.\n",
    "    output = []\n",
    "    print (\"word \",word)\n",
    "    try:\n",
    "      sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
    "      most_similar = s2v.most_similar(sense, n=topn)\n",
    "      # print (most_similar)\n",
    "      output = filter_same_sense_words(sense,most_similar)\n",
    "      print (\"Similar \",output)\n",
    "    except:\n",
    "      output =[]\n",
    "\n",
    "    threshold = 0.6\n",
    "    final=[word]\n",
    "    checklist =question.split()\n",
    "    for x in output:\n",
    "      if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
    "        final.append(x)\n",
    "\n",
    "    return final[1:]\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
    "    #  Maximal Marginal Relevance\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphrase\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\AASHUTOSH\n",
      "[nltk_data]     RAJ\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  Bitcoin\n",
      "NOUN\n",
      "Similar  ['Bitcoin', 'Cryptocurrency', 'Bitcoins', 'Bitcoins', 'Cryptos', 'Cryptocurrencies', 'Btc', 'Digital Currency', 'Crypto Currency', 'Bit Coin', 'Fiat', 'Crypto', 'Btc.', 'Altcoin', 'Altcoins', 'Btc', 'Litecoin', 'Coinbase', 'Crypto Currencies']\n",
      "distractors  ['Cryptocurrency', 'Cryptos', 'Btc', 'Digital Currency', 'Fiat', 'Coinbase']\n",
      "['Cryptocurrency', 'Fiat', 'Coinbase', 'Btc']\n",
      "time: 15 s (started: 2024-10-07 23:10:00 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def get_distractors_wordnet(word):\n",
    "  # function uses WordNet, a lexical database for the English language\n",
    "    distractors=[]\n",
    "    try:\n",
    "      syn = wn.synsets(word,'n')[0]\n",
    "\n",
    "      word= word.lower()\n",
    "      orig_word = word\n",
    "      if len(word.split())>0:\n",
    "          word = word.replace(\" \",\"_\")\n",
    "      hypernym = syn.hypernyms()\n",
    "      if len(hypernym) == 0:\n",
    "          return distractors\n",
    "      for item in hypernym[0].hyponyms():\n",
    "          name = item.lemmas()[0].name()\n",
    "          #print (\"name \",name, \" word\",orig_word)\n",
    "          if name == orig_word:\n",
    "              continue\n",
    "          name = name.replace(\"_\",\" \")\n",
    "          name = \" \".join(w.capitalize() for w in name.split())\n",
    "          if name is not None and name not in distractors:\n",
    "              distractors.append(name)\n",
    "    except:\n",
    "      print (\"Wordnet distractors not found\")\n",
    "    return distractors\n",
    "\n",
    "def get_distractors (word,origsentence,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
    "  # function combines the distractors obtained from \"sense2vec\"\n",
    "  # several inputs:\n",
    "  # word: The keyword for which distractors are to be generated.\n",
    "  # origsentence: The original sentence or question.\n",
    "  # sense2vecmodel: A \"sense2vec\" model for retrieving related words.\n",
    "  # sentencemodel: A sentence embedding model.\n",
    "  # top_n: The number of related words to retrieve using \"sense2vec.\"\n",
    "  # lambdaval: A parameter used in the MMR algorithm for diverse keyword selection.\n",
    "  distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
    "  print (\"distractors \",distractors)\n",
    "  if len(distractors) ==0:\n",
    "    return distractors\n",
    "  distractors_new = [word.capitalize()]\n",
    "  distractors_new.extend(distractors)\n",
    "  # print (\"distractors_new .. \",distractors_new)\n",
    "\n",
    "  embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
    "  # embedding_sentence = word\n",
    "  keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
    "  distractor_embeddings = sentencemodel.encode(distractors_new)\n",
    "\n",
    "  # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
    "  max_keywords = min(len(distractors_new),5)\n",
    "  filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
    "  # filtered_keywords = filtered_keywords[1:]\n",
    "  final = [word.capitalize()]\n",
    "  for wrd in filtered_keywords:\n",
    "    if wrd.lower() !=word.lower():\n",
    "      final.append(wrd.capitalize())\n",
    "  final = final[1:]\n",
    "  return final\n",
    "\n",
    "  # combines information from both \"sense2vec\" and WordNet to generate relevant and diverse distractors.\n",
    "  # MMR algorithm is used to select the most appropriate distractors based on relevance and diversity.\n",
    "\n",
    "sent = \"What cryptocurrency did Musk rarely tweet about?\"\n",
    "keyword = \"Bitcoin\"\n",
    "\n",
    "# sent = \"What did Musk say he was working with to improve system transaction efficiency?\"\n",
    "# keyword= \"Dogecoin\"\n",
    "\n",
    "\n",
    "# sent = \"What company did Musk say would not accept bitcoin payments?\"\n",
    "# keyword= \"Tesla\"\n",
    "\n",
    "\n",
    "# sent = \"What has Musk often tweeted in support of?\"\n",
    "# keyword = \"Cryptocurrency\"\n",
    "\n",
    "print (get_distractors(keyword,sent,s2v,sentence_transformer_model,40,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cheetah',\n",
       " 'Jaguar',\n",
       " 'Leopard',\n",
       " 'Liger',\n",
       " 'Saber-toothed Tiger',\n",
       " 'Snow Leopard',\n",
       " 'Tiger',\n",
       " 'Tiglon']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.09 s (started: 2024-10-07 23:10:30 +05:30)\n"
     ]
    }
   ],
   "source": [
    "get_distractors_wordnet('lion')\n",
    "# generate distractors for a given word by using WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we need computers in everything that we use in our daily lives. it becomes very important to make computers intelligent so that our lives become easy.\n",
      "artificial intelligence is the theory and development of computers, which imitates the human intelligence and senses, says dr. sanjay gupta. the u.s.\n",
      "government has backed the use of AI in the country.\n",
      "keywords unsummarized:  ['computers', 'artificial intelligence', 'languages', 'speech recognition', 'translation', 'lives', 'decision-making', 'development', 'theory', 'life', 'perception', 'senses', 'revolution', 'intelligence', 'world']\n",
      "keywords_found in summarized:  ['lives', 'theory', 'senses', 'intelligence', 'artificial intelligence', 'computers', 'development']\n",
      "\n",
      "\n",
      "Noun phrases ['computers', 'artificial intelligence', 'lives', 'development', 'theory', 'senses', 'intelligence']\n",
      "word  Computers\n",
      "NOUN\n",
      "Similar  ['Personal Computers', 'Desktop Computers', 'Home Computers', 'Computer Hardware', 'Own Computers', 'Supercomputers', 'Computer', 'Computer Systems', 'Just Computers', 'Laptops', 'Printers', 'Even Computers', 'Machines', 'Mainframes', 'Computers', 'Other Computers', 'Mobile Phones', 'Macs', 'Computing', 'Old Computers', 'First Computers', 'Actual Computers', 'More Computers', 'Own Computer', 'Real Computers', 'Ipads', 'Computer Stuff', 'Computer Tech', 'Super Computers', 'Game Consoles', 'Ipads', 'Modern Computers', 'New Computers', 'Smart Phones', 'Single Computer', 'Computer Programs', 'Software', 'Home Computer', 'Smartphones']\n",
      "distractors  ['Personal Computers', 'Computer Hardware', 'Computer Systems', 'Laptops', 'Printers', 'Machines', 'Mainframes', 'Mobile Phones', 'Macs', 'Own Computer', 'Ipads', 'Game Consoles', 'Smart Phones', 'Software']\n",
      "word  Artificial intelligence\n",
      "NOUN\n",
      "Similar  ['Strong Ai', 'Machine Intelligence', 'True Ai', 'Advanced Ai', 'Nanotechnology', 'Superintelligence', 'Artificial Intelligence', 'Ai Research', 'Sentient Ai', 'Human Intelligence', 'Ai Systems', 'Artificial Consciousness', 'Ai Technology', 'Quantum Computing', 'Technological Singularity', 'General Ai', 'Artificial Intelligence', 'Technology', 'Space Travel', 'Real Ai', 'Intelligent Machines', 'Human Augmentation', 'Super-Intelligence', 'Strong Ai', 'Computer Technology', 'Robotics', 'Future Technology', 'Strong Ai.', 'Intelligence Explosion', 'Super Intelligence', 'Complex Systems', 'True Artificial Intelligence', 'Human Technology', 'Artificial Intelligences', 'Neural Nets', 'Genetic Engineering', 'Human Consciousness']\n",
      "distractors  ['Strong Ai', 'True Ai', 'Advanced Ai', 'Nanotechnology', 'Superintelligence', 'Ai Research', 'Sentient Ai', 'Ai Systems', 'Artificial Consciousness', 'Quantum Computing', 'Technological Singularity', 'General Ai', 'Space Travel', 'Intelligent Machines', 'Human Augmentation', 'Computer Technology', 'Robotics', 'Intelligence Explosion', 'Complex Systems', 'Neural Nets', 'Genetic Engineering']\n",
      "word  Lives\n",
      "NOUN\n",
      "Similar  ['Lifes', 'Own Lives', 'Life', 'Peoples Lives', 'Young Lives', 'Own Life', 'Loved Ones', 'Entire Lives', \"Other People'S Lives\", 'Lives-', 'Fucking Lives', 'Individual Lives', 'Families', 'Livelihoods', 'Actual Lives', 'Own Families', 'Very Lives', 'Thier Lives', 'Human Lives', 'Other Lives', 'Daily Lives', 'Whole Lives', 'Lives*.', 'Future Lives', 'Persons Life', 'Many Lives', 'Innocent Lives', 'Horrific Ways', 'People Lives', 'Miserable Lives', 'Own Selves', 'Own Children', 'Childhoods', 'So Many Lives', 'Countless Families', \"Other Peoples' Lives\", 'Life', 'Shitty Lives']\n",
      "distractors  ['Own Lives', 'Peoples Lives', 'Loved Ones', 'Entire Lives', 'Fucking Lives', 'Individual Lives', 'Families', 'Livelihoods', 'Actual Lives', 'Thier Lives', 'Persons Life', 'Innocent Lives', 'Horrific Ways', 'Miserable Lives', 'Own Children', 'Childhoods', 'Countless Families']\n",
      "word  Development\n",
      "NOUN\n",
      "Similar  ['Further Development', 'Developement', 'Future Development', 'Actual Development', 'Continued Development', 'Development Process', 'Initial Development', 'Rapid Development', 'Developments', 'Early Development', 'Development Stage', 'Development Cycle', 'Very Early Stages', 'Current Development', 'Very Early Stage', 'Development', 'Own Development', 'Lifecycle', 'Development Phase', 'Continuous Development', 'New Technologies', 'Continuing Development', 'Developing', 'Major Development', 'Early Stages', 'Game Production', 'Internal Development', 'Improvements', 'Life-Cycle', 'Serious Development', 'Advancements', 'Dev Cycle', 'Development Stages', 'Significant Changes', 'Innovations', 'Roadmap', 'Full Development']\n",
      "distractors  ['Further Development', 'Development Process', 'Very Early Stages', 'Lifecycle', 'Continuous Development', 'New Technologies', 'Game Production', 'Improvements', 'Advancements', 'Dev Cycle', 'Significant Changes', 'Innovations', 'Roadmap']\n",
      "word  Theory\n",
      "NOUN\n",
      "Similar  ['Other Theory', 'Hypothesis', 'Other Theories', 'Only Theory', 'New Theory', 'Own Theory', 'Original Theory', 'Good Theory', 'Whole Theory', 'Theories', 'Specific Theory', 'Current Theory', 'Valid Theory', 'Actual Theory', 'Particular Theory', 'Alternate Theory', 'General Theory', 'Better Theory', 'Basic Assumption', 'Pet Theory', 'Just A Theory', 'Alternative Theory', 'First Theory', 'Working Theory', 'Best Explanation', 'Good Evidence', 'Simplest Explanation', 'Second Theory', 'Scientific Theory', 'Different Theory', 'Theoretical Basis', 'Best Evidence', 'Experimental Evidence', 'Basic Premise']\n",
      "distractors  ['Other Theory', 'Hypothesis', 'Original Theory', 'Good Theory', 'Specific Theory', 'Current Theory', 'Particular Theory', 'Alternate Theory', 'Basic Assumption', 'Just A Theory', 'Best Explanation', 'Good Evidence', 'Theoretical Basis', 'Experimental Evidence', 'Basic Premise']\n",
      "word  Senses\n",
      "NOUN\n",
      "Similar  ['Other Senses', 'Physical Senses', 'Sensory Input', 'Conscious Mind', 'Sensory Experiences', 'Subconsciousness', 'Unconscious Mind', 'Sensations', 'Own Senses', 'Sense Organs', 'Sensory Experience', 'Consciousness', 'Sensory Organs', 'Emotions', 'Sensory Information', 'Brain', 'Conscious Thought', 'External Stimuli', 'Mental Processes', 'Stimuli', 'Subconscious Mind', 'Conscious Thoughts', 'Emotional States', 'Own Brain', 'Sensory Perceptions', 'Brains', 'Certain Emotions', 'Subconscious Thoughts', 'Conscious State', 'Subjective Experience', 'Subconscious', 'Conciousness', 'Sensory Data', 'Sensory Inputs', 'Sensory Stimuli', 'Sensory Perception', 'Altered States', 'Conscious Awareness', 'Visual Stimuli']\n",
      "distractors  ['Other Senses', 'Physical Senses', 'Sensory Input', 'Conscious Mind', 'Sensory Experiences', 'Subconsciousness', 'Sensations', 'Sense Organs', 'Sensory Information', 'Brain', 'Conscious Thought', 'External Stimuli', 'Mental Processes', 'Stimuli', 'Emotional States', 'Own Brain', 'Sensory Perceptions', 'Certain Emotions', 'Subjective Experience', 'Altered States', 'Conscious Awareness']\n",
      "word  Intelligence\n",
      "NOUN\n",
      "Similar  ['Intellect', 'Intellectual Capacity', 'Intellegence', 'Actual Intelligence', 'Basic Intelligence', 'Intellectual Ability', 'Smartness', 'Inteligence', 'Social Intelligence', 'Superior Intelligence', 'Human Intelligence', 'Emotional Intelligence', 'Sophistication', 'Real Intelligence', 'Mental Ability', 'High Intelligence', 'More Intelligence', 'Reasoning Skills', 'Intellectual Prowess', 'Self-Awareness', 'Intellectual Capabilities', 'Reasoning Ability', 'Intelligence Level', 'Innate Intelligence', 'Superior Intellect', 'Competence', 'Overall Intelligence', 'Cognitive Ability', 'Natural Intelligence', 'Intellectual Capability', 'Abstract Thought', 'Own Intellect', 'Mental Capabilities', 'Resourcefulness', 'Self Awareness', 'Innate Ability', 'Brain Capacity', 'Critical Thinking Skills', 'General Intelligence']\n",
      "distractors  ['Intellectual Capacity', 'Smartness', 'Superior Intelligence', 'Sophistication', 'Mental Ability', 'Reasoning Skills', 'Self-Awareness', 'Competence', 'Cognitive Ability', 'Abstract Thought', 'Own Intellect', 'Resourcefulness', 'Brain Capacity', 'Critical Thinking Skills']\n",
      "What do we need in everything we use in our daily lives?\n",
      "Ans: Computers\n",
      "\n",
      "Mobile phones \n",
      "Macs \n",
      "Mainframes \n",
      "Ipads \n",
      "\n",
      "What is the term for the theory and development of computers?\n",
      "Ans: Artificial intelligence\n",
      "\n",
      "Space travel \n",
      "Human augmentation \n",
      "Superintelligence \n",
      "Technological singularity \n",
      "\n",
      "What does it become important to make computers intelligent so that our lives become easy?\n",
      "Ans: Lives\n",
      "\n",
      "Horrific ways \n",
      "Childhoods \n",
      "Families \n",
      "Livelihoods \n",
      "\n",
      "What is artificial intelligence?\n",
      "Ans: Development\n",
      "\n",
      "Significant changes \n",
      "New technologies \n",
      "Roadmap \n",
      "Lifecycle \n",
      "\n",
      "What is artificial intelligence?\n",
      "Ans: Theory\n",
      "\n",
      "Experimental evidence \n",
      "Basic assumption \n",
      "Best explanation \n",
      "Theoretical basis \n",
      "\n",
      "Along with human intelligence, what does artificial intelligence imitate?\n",
      "Ans: Senses\n",
      "\n",
      "Altered states \n",
      "Own brain \n",
      "Subconsciousness \n",
      "External stimuli \n",
      "\n",
      "What is the term for the theory and development of computers?\n",
      "Ans: Intelligence\n",
      "\n",
      "Self-awareness \n",
      "Abstract thought \n",
      "Sophistication \n",
      "Reasoning skills \n",
      "\n",
      "we need computers in everything that we use in our daily lives. it becomes very important to make computers intelligent so that our lives become easy. artificial intelligence is the theory and development of computers, which imitates the human intelligence and senses, says dr. sanjay gupta. the u.s. government has backed the use of AI in the country.\n",
      "time: 3min 10s (started: 2024-10-07 23:10:51 +05:30)\n"
     ]
    }
   ],
   "source": [
    "context = input(\"Enter paragraph/content here...\")\n",
    "output = \"\"\n",
    "# code starts by taking user input in the context variable\n",
    "\n",
    "def generate_question(context):\n",
    "  # function named generate_question that takes a context (paragraph or content) as input and generates a set of questions and their respective answer choices along with distractors.\n",
    "  summary_text = summarizer(context,summary_model,summary_tokenizer)\n",
    "  for wrp in wrap(summary_text, 150):\n",
    "    print (wrp)\n",
    "  np =  get_keywords(context,summary_text)\n",
    "  print (\"\\n\\nNoun phrases\",np)\n",
    "  output=\"\"\n",
    "  for answer in np:\n",
    "    ques = get_question(summary_text,answer,question_model,question_tokenizer)\n",
    "#  function first generates a summary of the input context using the summarizer function, which is defined elsewhere in your code\n",
    "#  summary is created using the summary_model and summary_tokenizer to compress the content into a shorter form.\n",
    "\n",
    "    output= output + ques + \"\\n\" + \"Ans: \"+answer.capitalize() + \"\\n\\n\"\n",
    "    distractors = get_distractors(answer.capitalize(),ques,s2v,sentence_transformer_model,40,0.2)\n",
    "    # Distractors are created using the get_distractors function, which combines information from \"sense2vec\" and WordNet to produce relevant distractors.\n",
    "\n",
    "    if len(distractors)>0:\n",
    "      for distractor in distractors[:4]:\n",
    "        output += distractor + \" \\n\"\n",
    "    output += \"\\n\"\n",
    "\n",
    "  summary =\"Summary: \"+ summary_text\n",
    "\n",
    "\n",
    "  output += summary_text\n",
    "\n",
    "\n",
    "  return output\n",
    "\n",
    "print(generate_question(context))\n",
    "# generate_question function with the user-provided context and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [08/Oct/2024 16:07:28] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords unsummarized:  ['tigers', 'lion', 'southern asia', 'wild', 'range', 'hunters', 'teeth', 'jaws', 'diet', 'mammal', 'relative', 'individuals', 'fact', 'bodies', 'spots']\n",
      "keywords_found in summarized:  ['hunters', 'jaws', 'tigers', 'teeth', 'wild', 'mammal', 'individuals', 'relative', 'bodies', 'lion']\n",
      "word  Tigers\n",
      "NOUN\n",
      "Similar  ['Leopards', 'Big Cats', 'Elephants', 'Cheetahs', 'Bears', 'Hippos', 'Gazelles', 'Crocodiles', 'Gorillas', 'Grizzly Bears', 'Antelopes', 'Sharks', 'Large Cats', 'Foxes', 'Otters', 'Deers', 'Giraffes', 'Polar Bears', 'Koalas', 'House Cats', 'Snakes', 'Coyotes', 'Jaguars', 'Other Predators', 'Orcas', 'Bulldogs', 'Other Big Cats', 'Scorpions', 'Large Animals', 'Hyenas', 'Lions', 'Wild Animals', 'Mammoths', 'Tiger', 'Snow Leopards', 'Zebras', 'Rabbits', 'Dolphins', 'Wolves', 'Hares']\n",
      "distractors  ['Leopards', 'Big Cats', 'Elephants', 'Cheetahs', 'Bears', 'Hippos', 'Gazelles', 'Crocodiles', 'Gorillas', 'Grizzly Bears', 'Antelopes', 'Sharks', 'Foxes', 'Otters', 'Giraffes', 'Polar Bears', 'Koalas', 'House Cats', 'Snakes', 'Coyotes', 'Jaguars', 'Other Predators', 'Orcas', 'Bulldogs', 'Other Big Cats', 'Scorpions', 'Large Animals', 'Hyenas', 'Lions', 'Mammoths', 'Zebras', 'Rabbits', 'Dolphins', 'Wolves']\n",
      "word  Lion\n",
      "NOUN\n",
      "Similar  ['Bear', 'Wolf', 'Lioness', 'Crocodile', 'Hippo', 'Tiger', 'Croc', 'Tusk', 'Sheep', 'Wildebeest', 'Hyenas', 'Grizzly Bear', 'Gazelle', 'Antelope', 'Cub', 'Centaur', 'Big Cat', 'Hyena', 'Otter', 'Swan', 'Polar Bear', 'Snake', 'Wolfs', 'Lizard', 'Mountain Lion', 'Cheetah', 'Male Lion', 'Rat', 'Giraffe', 'Turtle', 'Rhinoceros', 'House Cat', 'Rabbit', 'Wolves', 'Grizzly', 'Elephant', 'Squirrel', 'Simba', 'Peacock']\n",
      "distractors  ['Bear', 'Wolf', 'Lioness', 'Crocodile', 'Hippo', 'Tiger', 'Croc', 'Tusk', 'Sheep', 'Wildebeest', 'Hyenas', 'Grizzly Bear', 'Gazelle', 'Antelope', 'Cub', 'Centaur', 'Big Cat', 'Otter', 'Swan', 'Polar Bear', 'Snake', 'Lizard', 'Mountain Lion', 'Cheetah', 'Male Lion', 'Rat', 'Giraffe', 'Turtle', 'Rhinoceros', 'House Cat', 'Rabbit', 'Wolves', 'Grizzly', 'Elephant', 'Squirrel', 'Simba', 'Peacock']\n",
      "word  Wild\n",
      "NOUN\n",
      "Similar  ['Flames', 'Blues', 'Wings', 'Crow', 'Red', 'Buffalo', 'Devils', 'Bolts', 'Miracle', 'Lightening', 'Avalanche', 'Golden', 'Crush', 'Beasts', 'Kings', 'Bayou', 'Away', 'Rays', 'Lightning', 'Pond', 'Dawn', 'Sleeper', 'Wicked', 'Stray', 'Phoenix', 'Bitter', 'Calgary', 'Hawk', 'Sea', 'Shark', 'Cut', 'Nashville', 'Giant']\n",
      "distractors  ['Flames', 'Blues', 'Wings', 'Crow', 'Red', 'Buffalo', 'Devils', 'Bolts', 'Miracle', 'Lightening', 'Avalanche', 'Golden', 'Crush', 'Beasts', 'Bayou', 'Away', 'Rays', 'Pond', 'Dawn', 'Sleeper', 'Wicked', 'Stray', 'Phoenix', 'Bitter', 'Calgary', 'Hawk', 'Sea', 'Cut', 'Nashville', 'Giant']\n",
      "word  Hunters\n",
      "NOUN\n",
      "Similar  ['Hunters', 'Other Hunters', 'Most Hunters', 'Trappers', 'Only Hunters', 'Warlocks', 'Many Hunters', 'Just Hunters', 'More Hunters', 'Hunter Class', 'Few Hunters', 'Other Hunter', 'Necros', 'Wraith', 'Good Hunters', 'Ferals', 'Guardians', 'Rogues', 'Thrall', 'Single Hunter', 'Melees', 'Most Hunters', 'Good Hunter', 'Only Hunter', 'Warlocks', 'Monster Players', 'Vamps', 'Minotaurs', 'Druids', 'Best Hunter', 'Titans', 'Monsters', 'Trapper', 'Warriors', 'Paladins']\n",
      "distractors  ['Other Hunters', 'Trappers', 'Warlocks', 'Hunter Class', 'Necros', 'Wraith', 'Ferals', 'Guardians', 'Rogues', 'Thrall', 'Single Hunter', 'Melees', 'Good Hunter', 'Monster Players', 'Vamps', 'Minotaurs', 'Druids', 'Titans', 'Warriors', 'Paladins']\n",
      "word  Teeth\n",
      "NOUN\n",
      "Similar  ['Own Teeth', 'Gums', 'Back Teeth', 'Front Teeth', 'Finger Nails', 'Fingernails', 'Bottom Teeth', 'Molars', 'Dogs Teeth', 'Gum Line', 'Nails', 'Top Teeth', 'Nose', 'Remaining Teeth', 'Teeths', 'Fucking Teeth', 'Brushing', 'Whole Mouth', 'Upper Lip', 'Scabs', 'Broken Teeth', 'Little Teeth', 'Loose Teeth', 'Toenails', 'Ear Lobe', 'Dentures', 'Gumline', 'Entire Mouth', 'Lower Front Teeth', 'Bite Marks', 'Lower Lip']\n",
      "distractors  ['Own Teeth', 'Gums', 'Finger Nails', 'Bottom Teeth', 'Molars', 'Gum Line', 'Nails', 'Nose', 'Remaining Teeth', 'Brushing', 'Whole Mouth', 'Upper Lip', 'Scabs', 'Ear Lobe', 'Dentures', 'Entire Mouth', 'Lower Front Teeth', 'Bite Marks']\n",
      "word  Jaws\n",
      "NOUN\n",
      "Similar  ['Powerful Jaws', 'Jaw', 'Sharp Teeth', 'Beak', 'Lower Jaw', 'Soft Flesh', 'Sharp Claws', 'Back Feet', 'Talons', 'Long Claws', 'Fangs', 'Mandibles', 'Appendages', 'Neck', 'Claws', 'Windpipe', 'Ribcage', 'Hind Legs', 'Still-Beating Heart', 'Limbs', 'Front Legs', 'Snout', 'Leg', 'Paw', 'Haunches', 'Teeth', 'Clenched Fist', 'Own Arm', 'Hindquarters', 'Arms', 'Upper Jaw', 'S Neck', 'Legs', 'Jugular', 'Rear Legs', 'Sternum', 'Tail', 'Necks']\n",
      "distractors  ['Powerful Jaws', 'Sharp Teeth', 'Beak', 'Soft Flesh', 'Sharp Claws', 'Back Feet', 'Talons', 'Long Claws', 'Fangs', 'Mandibles', 'Appendages', 'Neck', 'Windpipe', 'Ribcage', 'Hind Legs', 'Still-Beating Heart', 'Limbs', 'Snout', 'Leg', 'Paw', 'Haunches', 'Teeth', 'Clenched Fist', 'Own Arm', 'Hindquarters', 'Arms', 'Upper Jaw', 'S Neck', 'Jugular', 'Rear Legs', 'Sternum', 'Tail']\n",
      "word  Mammal\n",
      "NOUN\n",
      "Similar  ['Mammals', 'Other Animal', 'Primate', 'Insect', 'Rodent', 'Platypus', 'Other Mammal', 'Most Mammals', 'Land Animal', 'Animal', 'Certain Species', 'Reptile', 'Only Animal', 'Only Animals', 'Species', 'Particular Species', 'Capybara', 'Amphibian', 'Most Species', 'Other Mammals', 'Vertebrates', 'Marsupial', 'Only Mammal', 'Single Species', 'Bovine', 'Only Species', 'Several Species', 'Most Animals', 'Vertebrate', 'Other Animals', 'Many Animals', 'Invertebrates', 'Only Mammals', 'Jellyfish', 'Crustaceans']\n",
      "distractors  ['Other Animal', 'Primate', 'Insect', 'Rodent', 'Platypus', 'Most Mammals', 'Land Animal', 'Animal', 'Certain Species', 'Reptile', 'Only Animals', 'Species', 'Particular Species', 'Capybara', 'Amphibian', 'Vertebrates', 'Marsupial', 'Single Species', 'Bovine', 'Jellyfish', 'Crustaceans']\n",
      "word  Relative\n",
      "NOUN\n",
      "Similar  ['Other Relative', 'Family Member', 'Father', 'Family', 'Brother-In-Law', 'Family Friend', 'Grandmother', 'Mother', 'Stepfather', 'Close Friend', 'Aunt', 'Cousin', 'Step Father', 'Uncle', 'Stepdad', 'Grandfather', 'Good Family Friend', 'Brother', 'Grandparent', 'Father-In-Law', 'Close Family Friend', 'Relatives', 'Immediate Family', 'Other Family Member', 'Acquaintance', 'Dad', 'Step-Father', 'Sibling', 'Other Brother', 'Friend', 'Sister-In-Law', 'Ex-Husband', 'Similar Person', 'Sister', 'Very Close Friend', 'Step-Mother']\n",
      "distractors  ['Other Relative', 'Family Member', 'Father', 'Family', 'Brother-In-Law', 'Family Friend', 'Grandmother', 'Close Friend', 'Aunt', 'Cousin', 'Step Father', 'Uncle', 'Stepdad', 'Brother', 'Grandparent', 'Immediate Family', 'Acquaintance', 'Dad', 'Sibling', 'Other Brother', 'Friend', 'Ex-Husband', 'Similar Person', 'Sister']\n",
      "word  Individuals\n",
      "NOUN\n",
      "Similar  ['Many Individuals', 'Other Individuals', 'Individuals', 'Certain Individuals', 'Individual People', 'Individual', 'Such Individuals', 'Few Individuals', 'Only Individuals', 'Specific Individuals', 'Certain Groups', 'Persons', 'Single Individual', 'Most Individuals', 'Specific Groups', 'Individuals', 'Just Individuals', 'Individual Members', 'Particular Groups', 'Particular Group', 'Collective', 'Single Individuals', 'Certain Group', 'Particular Individual', 'Particular Individuals', 'Particular People', 'Organizations', 'Societies', 'Institutions', 'Society', 'Individual Humans', 'Such Groups', 'Individual Person', 'Individual People', '&Gt;Individuals', 'Certain People', 'Larger Society', 'Collective Group', 'Specific Group', 'Individual Level']\n",
      "distractors  ['Certain Individuals', 'Individual People', 'Certain Groups', 'Persons', 'Single Individual', 'Particular Groups', 'Collective', 'Organizations', 'Societies', 'Institutions', 'Such Groups', 'Larger Society', 'Specific Group']\n",
      "word  Bodies\n",
      "NOUN\n",
      "Similar  ['Human Bodies', 'Body Parts', 'Bodys', 'Organs', 'Stomachs', 'Whole Bodies', 'Own Bodies', 'Body', 'Internal Organs', 'Brains', 'Physical Bodies', 'Entire Bodies', 'Actual Bodies', 'Corpses', 'Peoples Bodies', 'Dead Bodies', 'Reproductive Organs', 'Bellies', 'Wombs', 'Sex Organs', 'Own Heads', 'Nervous Systems', 'Bodily Functions', 'Body-', 'Living People', 'Different Bodies', 'Living Creatures', 'Natural Mechanisms', 'Cells', 'Living Humans', 'Living Beings', 'Newborn Babies', 'Living Things', 'Own Body', 'Biological Functions', 'Human Beings', 'Limbs', 'Humans', 'Little Bodies', 'Rotting Corpses']\n",
      "distractors  ['Human Bodies', 'Body Parts', 'Organs', 'Stomachs', 'Whole Bodies', 'Body', 'Internal Organs', 'Brains', 'Corpses', 'Reproductive Organs', 'Bellies', 'Wombs', 'Own Heads', 'Nervous Systems', 'Bodily Functions', 'Living People', 'Different Bodies', 'Living Creatures', 'Natural Mechanisms', 'Cells', 'Living Humans', 'Newborn Babies', 'Own Body', 'Humans', 'Rotting Corpses']\n",
      "[{'question': 'What is a powerful hunter?', 'answer': 'tigers', 'options': ['tigers', 'Orcas', 'Scorpions', 'Polar bears']}, {'question': 'What is the closest relative of a tiger?', 'answer': 'lion', 'options': ['lion', 'Tiger', 'Hippo', 'Simba']}, {'question': 'Researchers can use stripe patterns to identify different individuals when studying animals in what?', 'answer': 'wild', 'options': ['wild', 'Sleeper', 'Crow', 'Lightening']}, {'question': 'What is the job of a tiger?', 'answer': 'hunters', 'options': ['hunters', 'Thrall', 'Melees', 'Minotaurs']}, {'question': 'What do tigers have sharply?', 'answer': 'teeth', 'options': ['teeth', 'Brushing', 'Scabs', 'Gum line']}, {'question': \"What part of a tiger's body is strong?\", 'answer': 'jaws', 'options': ['jaws', 'Powerful jaws', 'Still-beating heart', 'Haunches']}, {'question': 'What is the closest relative of a lion?', 'answer': 'mammal', 'options': ['mammal', 'Jellyfish', 'Platypus', 'Bovine']}, {'question': \"What is a lion's closest relative?\", 'answer': 'relative', 'options': ['relative', 'Immediate family', 'Ex-husband', 'Other brother']}, {'question': 'Researchers can use stripe patterns to identify what?', 'answer': 'individuals', 'options': ['individuals', 'Certain individuals', 'Larger society', 'Institutions']}, {'question': 'What part of a tiger is agile?', 'answer': 'bodies', 'options': ['bodies', 'Body parts', 'Natural mechanisms', 'Newborn babies']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Oct/2024 16:15:30] \"\u001b[37mPOST /generate_questions HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9min 11s (started: 2024-10-08 16:07:22 +05:30)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "from srsly import json_dumps\n",
    "\n",
    "# Create the Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    # Render the input form\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/generate_questions', methods=['POST'])\n",
    "def generate_questions():\n",
    "    # Get the user input from the form\n",
    "    context = request.form['paragraph']\n",
    "    \n",
    "    # Process the input using the predefined summarizer and models\n",
    "    summary_text = summarizer(context, summary_model, summary_tokenizer)\n",
    "    \n",
    "    # Wrapping the summary text (if necessary)\n",
    "    wrapped_summary = [wrp for wrp in wrap(summary_text, 150)]\n",
    "    \n",
    "    # Extract keywords (noun phrases) from the summary\n",
    "    np = get_keywords(context, summary_text)\n",
    "    \n",
    "    arr = []\n",
    "    for answer in np:\n",
    "        # Generate the question for each noun phrase\n",
    "        ques = get_question(summary_text, answer, question_model, question_tokenizer)\n",
    "        \n",
    "        # Get distractors for multiple-choice options\n",
    "        distractors = get_distractors(answer.capitalize(), ques, s2v, sentence_transformer_model, 40, 0.2)\n",
    "        \n",
    "        # Append the question, correct answer, and distractors to the array\n",
    "        question_data = {\n",
    "            \"question\": ques,\n",
    "            \"answer\": answer,\n",
    "            \"options\": [answer] + distractors[:3]  # Take up to 3 distractors\n",
    "        }\n",
    "        arr.append(question_data)\n",
    "    \n",
    "    print(arr)  # Debug: Print the array to ensure it has the right structure\n",
    "    \n",
    "    # Return the data as JSON\n",
    "    return jsonify(arr)\n",
    "\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
